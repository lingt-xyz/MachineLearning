# Generalization

### [Peril of Overfitting](https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting)

Creating a model that matches the training data so closely that the model fails to make correct predictions on new data.

### The ML fine print

The following three basic assumptions guide generalization:

* We draw examples **independently and identically** \(i.i.d\) at random from the distribution. In other words, examples don't influence each other. \(An alternate explanation: i.i.d. is a way of referring to the randomness of variables.\)
* The distribution is **stationary**; that is the distribution doesn't change within the data set.
* We draw examples from partitions from the **same distribution**.

